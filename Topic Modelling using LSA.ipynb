{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7341cf73",
   "metadata": {},
   "source": [
    "# Topic Modelling using LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e3bd6",
   "metadata": {},
   "source": [
    "Algoritma LSA (Latent Semantic Analysis) adalah salah satu algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase/kalimat dengan sekumpulan dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd96963",
   "metadata": {},
   "source": [
    "Pada program ini akan menggunakan data abstrak dari portal tugas akhir trunojoyo program studi Teknik Informatika (https://pta.trunojoyo.ac.id/c_search/byprod/10), berikut code untuk melakukan crawling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245741b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install library beautifulsoup4 untuk melakukan crawling data\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# membuat list, dataAbstract untuk menampung data sementara setelah crawling\n",
    "# dataFix untuk menampung data yang sudah ditambahkan kolom index dan siap di convert ke csv\n",
    "dataAbstract = []\n",
    "dataFix = []\n",
    "\n",
    "# function crawlAbstract untuk mengambil data judul dan abstract dari halaman detail pta trunojoyo teknik informatika\n",
    "def crawlAbstract(src):\n",
    "    # inisialisasi beautifulsoup4     \n",
    "    global c\n",
    "    tmp = []\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mengambil data judul     \n",
    "    title = soup.find(class_=\"title\").getText()\n",
    "    tmp.append(title)\n",
    "    \n",
    "    # mengambil data abstract     \n",
    "    abstractText = soup.p.getText()\n",
    "    tmp.append(abstractText)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# function getLinkToAbstract digunakan untuk mengambil data link menuju halaman detail\n",
    "# parameter src berisi link halaman daftar tugas akhir\n",
    "def getLinkToAbstract(src):\n",
    "    # inisialisasi beautifulsoup4\n",
    "    global c\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mendapatkan semua link menuju halaman detail\n",
    "    items = soup.find(class_=\"items\").find_all('a')\n",
    "    # looping setiap link untuk mendapatkan nilai href, \n",
    "    # link tersebut digunakan sebagai parameter function crawlAbstract agar mendapat data judul dan abstract\n",
    "    for item in items:\n",
    "        if item.get('href') != '#':\n",
    "            tmp = crawlAbstract(item.get('href'))\n",
    "            # dataAbstract menampung data sementara hasil crawl\n",
    "            dataAbstract.append(tmp)\n",
    "\n",
    "\n",
    "# link halaman pta trunojoyo prodi teknik informatika yang akan di crawl\n",
    "# halaman ini berisi daftar tugas akhir\n",
    "link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10\"\n",
    "# mengambil data sampai halaman 100\n",
    "for i in range(1, 101):\n",
    "    # memindah halaman menuju halaman selanjutnya     \n",
    "    src = f\"https://pta.trunojoyo.ac.id/c_search/byprod/10/{i}\"\n",
    "    # counter untuk melihat progress berapa persen proses crawling\n",
    "    print(f\"Proses-{i}%\")\n",
    "    # memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\n",
    "    getLinkToAbstract(src)\n",
    "\n",
    "# setelah memperoleh semua data abstract, data tersebut ditampung di list dataAbstract\n",
    "# data perlu ditambahkan kolom index sebagai id\n",
    "# looping berikut bertujuan menambahkan kolom index di setiap baris, lalu disimpan di list dataFix\n",
    "for i in range(1, len(dataAbstract)+1):\n",
    "    dataAbstract[i-1].insert(0, i)\n",
    "    dataFix.append(dataAbstract[i-1])\n",
    "\n",
    "# menyimpan data hasil crawl dengan format csv\n",
    "header = ['index', 'title','abstract']\n",
    "with open('dataHasilCrawl.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataFix)\n",
    "# akan ada file dataHasilCrawl.csv berisi id, judul dan abtrak dari pta trunojoyo teknik informatika sejumlah 500 record\n",
    "# proses crawling selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c254a0a",
   "metadata": {},
   "source": [
    "Tahap selanjutnya melakukan pre-processing data dengan tahapan 1.punctuation removal 2.stemming\n",
    "1. punctuation removal adalah proses membersihkan teks dari tanda baca dan angka\n",
    "2. Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Sederhananya, proses mengubah kata berimbuhan menjadi kata dasar, misal kata \"membosankan\" menjadi \"bosan\"\n",
    "3. Stopwords merupakan kata yang diabaikan dalam pemrosesan karena termasuk kata umum yang mempunyai fungsi tapi tidak mempunyai arti.Maksud dari kata umum adalah kata yang frekuensi kemunculannya tinggi, misalnya kata penghubung seperti “dan”, “atau”, “tapi”, “akan” dan lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2eb32",
   "metadata": {},
   "source": [
    "Install terlebih dahulu library yang akan digunakan:\n",
    "Sastrawi digunakan untuk proses stemming dan stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282ffc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # untuk menyimpan hasil dalam format csv\n",
    "import string \n",
    "import re # re : digunakan untuk proses punctuation removal\n",
    "\n",
    "# memanggil function yang digunakan\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# membuat list untuk menampung data\n",
    "dataAbstract = []\n",
    "dataAfterPreprocessing = []\n",
    "\n",
    "# inisialisasi library sastrawi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# inisialisasi library sastrawi untuk proses stopword removal\n",
    "factory2 = StopWordRemoverFactory()\n",
    "stopword = factory2.create_stop_word_remover()\n",
    "\n",
    "# untuk counter proses\n",
    "count = 1\n",
    "\n",
    "# membaca data dari proses sebelumnya\n",
    "with open(\"dataHasilCrawl.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        if len(row) != 0:\n",
    "#           data sebelum proses disimpan pada list dataAbstract\n",
    "            dataAbstract.append(row)\n",
    "\n",
    "# looping untuk memproses setiap data\n",
    "for abstract in dataAbstract:\n",
    "#   ambil data\n",
    "    tmp = abstract.pop()\n",
    "#   lakukan case folding (mengubah teks menjadi bentuk standar: huruf kecil)\n",
    "    tmp = tmp.lower()\n",
    "#   menghapus angka\n",
    "    tmp = re.sub(r\"\\d+\", \"\", tmp)\n",
    "#   menghapus tanda baca\n",
    "    tmp = tmp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "#   menghapus whitespace\n",
    "    tmp = tmp.strip()\n",
    "    tmp = re.sub('\\s+',' ',tmp)\n",
    "#   melakukan proses stemming\n",
    "    tmp = stemmer.stem(tmp)\n",
    "#   melakukan proses stopword removal\n",
    "    tmp = stopword.remove(tmp)\n",
    "#   menambahkan data ke list dataAfterPreprocessing\n",
    "    abstract.append(tmp)\n",
    "    dataAfterPreprocessing.append(abstract)\n",
    "#   print counter proses\n",
    "    print(f\"Proses:{count}/{len(dataAbstract)}\")\n",
    "    count+=1\n",
    "\n",
    "# menyimpan data dari list dataAfterPreprocessing ke bentuk csv\n",
    "header = ['index', 'title','abstract_cleaned']\n",
    "with open('dataAfterPreprocessing.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataAfterPreprocessing)\n",
    "# akan ada file dataAfterPreprocessing.csv berisi id, judul, abtract yang sudah dipreprocessing\n",
    "# preprocessing sudah selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f0ff1",
   "metadata": {},
   "source": [
    "Masuk ke tahap penerapan LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a57884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
